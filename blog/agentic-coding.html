<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Agentic Coding: A New Abstraction Layer - Michael Rizvi-Martel</title>
    <link rel="stylesheet" href="../styles.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.css">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.js"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/contrib/auto-render.min.js"
        onload="renderMathInElement(document.body);"></script>
</head>
<body>
    <article class="blog-post">
        <header class="blog-header">
            <a href="../index.html" class="blog-home-link">Michael Rizvi-Martel</a>
            <h1 class="blog-title">Agentic Coding: A New Abstraction Layer in the Programming Stack</h1>
            <p class="blog-subtitle">Or How I Learned to Stop Worrying About AGI and Love Agents</p>
            <p class="blog-date">February 2026</p>
        </header>

        <div class="blog-content">
            <p class="blog-tldr"><strong>TLDR:</strong>
            I give some thoughts on how AI has influenced my coding setup throughout the years since ChatGPT's release.
            I argue that agents can be thought of as a new layer of abstraction in the programming stack, speculating (with some uncertainty) that future software developers will "code" requirements in natural language. (I am not sure what form this will take; what the "IDEs of the future" will look like.)
            I present a historical comparison of the mistrust at the advent of new layers of abstraction in the stack, motivating the previous point and arguing that the resistance to coding agents is similar to that of other major paradigm shifts in programming.</p>

            <h2>Agents and Me</h2>

            <p>Agents have taken the world by storm. I think it is fair to say that no one in the programming world has been left indifferent by their advent. Although these systems have not made their way into every workplace and every developer's computer yet, they have their fair share of early adopters. Across conversations with AI researcher and developer friends, I have really seen a range of opinions. Some PhD colleagues flat out refuse to use them out of mistrust: they feel like understanding the internals of these models makes them less trusting — like aviation engineers being scared to take the plane, knowing every possible thing that could go wrong. Others, like myself at this point, have totally leaned into the coding agents, for better or for worse. I have been using Claude Code with the <code>--dangerously-skip-all-permissions</code> flag active all the time (meaning it no longer asks me for permission to run commands) and have fully leaned into the "skills" and CLAUDE.md development philosophy. Customizing my Claude Code setup reminds me of how I would customize my IDE back when I started grad school. To be quite honest, agentic coding has made me love building again. I feel like the barrier to entry to forking a codebase or starting a new one from scratch is much lower. It is hard to say how much of a productivity gain coding with agents has brought me, but I am certain that I have saved myself a lot of time spent with the "blank page syndrome," wondering how to structure a codebase or just procrastinating starting by lack of motivation. Agentic systems might not always be right (however, being honest, they are right a lot of the time), but starting from a first draft, or saving the pain of reading through someone else's code to start a new project is definitely freeing for me. Going back to this IDE vs. CLAUDE.md thing, I noticed more broadly that, throughout my budding academic career, I can chart my software development paradigm shifts into four broad categories:</p>

            <ol>
                <li><strong>Coding + Stack Overflow:</strong> from undergrad to early ChatGPT days, I would code manually and go to Stack Overflow to find functions or one-liners to solve specific problems. I would typically organize my code (e.g., a main or a complicated object/function) by first writing the headers and writing comments in natural language explaining what each subpart of the code would do. Then I would either code each "comment" or go look for a Stack Overflow post or such to fill it in.</li>
                <li><strong>Coding + ChatGPT:</strong> The first "AI revolution" for me was the shift from Stack Overflow to ChatGPT. When ChatGPT got good enough at writing Python code, I started using it the same way I would use Stack Overflow. The rest of the development remained the same. Through this phase, I started using ChatGPT to generate more and more ambitious bits of code, giving it more and more freedom (from one or two lines of code to writing full functions/methods for an object).</li>
                <li><strong>Coding + Copilot:</strong> At first this was similar to ChatGPT use, writing comments and having Copilot autocomplete the missing bits according to the spec. The magic here was that I could write the comments and have Copilot directly add the lines of code in the IDE! At this phase I started dabbling with agents in the Copilot setup. I was not convinced though; at that point Copilot's agentic setup was going too fast for me and was making a fair bit of mistakes. I felt that I needed to give more supervision to the AI I was letting code.</li>
                <li><strong>Agents (Claude Code in my case):</strong> I finally decided to <a href="https://arxiv.org/abs/2512.14806">let the barbarians in</a> in the summer of 2025. A friend of mine at ACL said "You have to try Claude Code man, it's <em>really</em> good" in response to my skepticism concerning agents. I did, and I have not looked back. Depending on the nature of the project, I sometimes don't even look at the code anymore. The more I use it, the more I realize how good it is. I am not the best programmer, but I am not a terrible one either. In any case, Claude is definitely a better programmer than I am.</li>
            </ol>

            <h2>Agents as a New Layer of Abstraction</h2>

            <p>Going through these shifts in my software development philosophy got me thinking about the nature of software development. Which is, namely, about developing software. In terms of a black box, we have something like "requirements in → software out." Throughout most of the history of programming, computer scientists developed new layers of abstraction to facilitate the development of code. At the current point of my ridiculous vibe coding follies, I figured that we are at the cusp of a new layer of abstraction.</p>

            <p>Andrej Karpathy tweeted a couple years back that <a href="https://x.com/karpathy/status/1617979122625712128">"The hottest new programming language is English."</a> This could not be truer today in the era of coding agents. I code in English now. Claude does the technical part for me. But this does not mean that I code mindlessly. Throughout all the tokens I burned, I noticed a principle which I dub "information in, information out": although I write mostly in English and can abstract away algorithmic details (<em>how</em> the tensor operation will be done, <em>what</em> specific method to use for something), I have to be clear about <em>design decisions</em>. What loss function to use? How to structure the data tensor (batch first or not)? What positional encodings for the transformer? Although these are not low-level technical details, these design choices still require some level of technical competency. When I leave these unspecified, the model typically does it wrong; using an incorrect loss function or inferring some technical detail of the architecture different from what I want. And how could it know? Especially in research and when it comes to the "art of training deep models," there is no objective right answer to these questions.</p>

            <p>To make this concrete, here is a rough example from my actual workflow. Say I want to train a small transformer on a sequence copying task (the kind of toy task you would use to study length generalization). If I just tell Claude Code "write me a transformer that learns to copy sequences," I will get <em>something</em> — but it will make a bunch of design choices that I probably disagree with. It might use learned positional encodings when I wanted NoPE or RoPE. It might set up the causal mask wrong for an encoder-decoder vs. a decoder-only setup. It might default to cross-entropy when the task structure calls for something else. The code will run, but it won't be <em>my</em> experiment. What I actually need to specify is closer to: "decoder-only transformer, RoPE embeddings, 4 layers, 256 hidden dim, trained on sequence copying where input is <code>[tokens] [SEP]</code> and target is the same tokens again, use teacher forcing, cross-entropy loss only on the target portion of the sequence, train lengths 1–20 and evaluate on lengths 21–40." Once I give it that, it gets it right almost every time. The implementation details — how to build the RoPE rotation matrices, how to set up the data loader, how to mask the loss — those it handles fine on its own. The point is: the things I still need to specify are design-level decisions, not implementation-level ones. The abstraction eats the implementation but not the design.</p>

            <p>I think this maps pretty cleanly onto how previous layers of abstraction worked. Each one removed a category of decisions from the programmer's plate while keeping the ones above it.
            When C came along, you no longer had to think about register allocation or instruction
            scheduling, but you still had to manage memory — <code>malloc</code>, <code>free</code>, pointer arithmetic, all of that was still on you. When Java and managed runtimes appeared, memory management got abstracted away by garbage collection, but you still had to think about your object structure, your type hierarchy, how to decompose the system. When scripting languages like Python took off, you could stop worrying about type declarations and boilerplate, but you still had to think about the algorithm, the control flow, the architecture of your code. Agents, I think, are the next step in this progression. They abstract away the implementation — the specific syntax, the library calls, the boilerplate — but the design decisions stay with you. In the DL case: the model architecture, the training procedure, the evaluation protocol. In web dev, I imagine it would be something like the database schema or the API design. The stuff that requires <em>judgment</em> rather than <em>knowledge of how to write it down</em>.</p>

            <p>This led me to speculate about the future of software development (or computer use in general). Although speculation often turns out to be wrong, I felt like I would give it a go anyway. I figure if my reflections are high-level enough, I can retroactively justify them as being right in hindsight à la Schmidhuber (who probably has already scooped my blog post idea...). I should note that I am an ML researcher, not a software developer; my experience with agents is primarily in the context of research codebases and deep learning experiments, not production software. I will talk about AI mostly as I don't have much recent experience developing other things, but I think a lot of the ideas transfer.</p>

            <p>As it stands, we now have a "design requirements in natural language in → code out" pipeline. That being said, the way I interact with Claude Code is actually not that different from how I used to code before AI was in the picture. Back in undergrad, my workflow was to write out the structure of my code in comments first — natural language descriptions of what each block should do — and then fill in the actual code underneath each comment (or go find it on Stack Overflow). The agentic workflow is the same thing, except now I never write the code underneath the comments. The comments <em>are</em> the program. The filling-in is handled by the agent. In a way, the paradigm shift is less radical than it seems: I was already programming in a mix of natural language and code, I just didn't have a system that could execute the natural language part.</p>

            <p>If I had to guess where this goes next, I think there will be something like a convergence on how to write good "specs" for agents. Every field of programming already has its own terminology and conventions, and these are probably the natural starting point. A DL researcher prompting an agent and a frontend developer prompting an agent are going to use very different vocabularies, but in both cases the quality of the output is going to depend heavily on how precisely you specify the design-level decisions. I figure the workflow will eventually settle into something like two parts: first you detail the specs — inputs, outputs, design choices, constraints — and then you give a high-level pseudocode of what you want the system to do. Not implementation-level pseudocode, but something more like a description of the data flow and the major components. I don't know exactly what form this will take or what the "IDEs of the future" will look like, but I think the direction is fairly clear.</p>

            <p>There is, however, one fundamental difference between this abstraction layer and every one that came before it: it is <em>inherently stochastic</em>. When a C compiler compiles your code, it produces the same assembly every time for the same input. When the JVM runs your bytecode, the behavior is deterministic. When you write a Python script, it does the same thing on every run (modulo random seeds and such). Agents don't work this way. Ask Claude Code the same thing twice and you might get different implementations. Sometimes meaningfully different. This is, I think, the core reason the trust problem with agents feels different from the trust problem with compilers or garbage collectors. With a compiler, once you've verified it works, you can trust it on new inputs because the mapping from source to output is fixed. With an agent, every invocation is in some sense a new roll of the dice.</p>

            <p>That being said, no two human programmers would implement <em>exactly</em> the same program, even if given the same spec. In the case of humans, as long as the code is efficient and it passes the required tests, we typically don't have a problem with this. This leads me to believe that the future of coding is going to be much more test-driven. In my own experience, as long as I scrutinize the outputs of Claude's code enough, it eventually finds and fixes the bug. For research, this is through looking at plots or data and following up with Claude about these things: "Isn't it weird that the model is training so slow?" "How come metric X is so low when metric Y is quite high?" For frontend development, given we can literally see the results, I would figure this output-driven development would also be feasible. For production-level backend code, where bugs can be silent and the stakes are higher, adoption will likely lag. But if the history of programming is any guide, that is exactly how every new abstraction starts.</p>

            <h2>A Historical Outlook</h2>

            <p>As is the case with most paradigm shifts, there is resistance to change. I have seen this myself (see intro) and I figure most of this mistrust is not unfounded. An interesting way to see we are in a paradigm shift is to look back, which I will try to do in this section. Previous major shifts in programming showed a similar resistance-to-change timeline. For instance, when compilers were introduced, many senior programmers refused to use them, stating that they could optimize instructions better than the machine. They weren't totally wrong: at the time, early compilers <em>were</em> producing worse code than a skilled assembly programmer could write by hand. John Backus, the creator of Fortran, described the programmers of the 1950s as <a href="https://dl.acm.org/doi/10.1145/960118.808380">"members of a priesthood guarding skills and mysteries far too complex for ordinary people"</a> who met the idea of compilers with "considerable hostility and derision." Even von Neumann himself — the father of the stored-program computer — reportedly dismissed the idea of compilers: <a href="https://history.computer.org/pioneers/von-neumann.html">"Why would you want more than machine language?"</a>. Grace Hopper built the first compiler (A-0) in 1952 and <a href="https://en.wikipedia.org/wiki/Grace_Hopper">nobody would touch it for three years</a>. She was told computers could only do arithmetic. She had a <em>working compiler</em> and people still didn't believe it. In a way, there is something deeply human about this. Not only was this a matter of trust ("can the machine really do what I do?"), but also of identity ("if the machine can do what I do, what am I?"). There is a famous <a href="https://users.cs.utah.edu/~elb/folklore/mel.html">Usenet poem from 1983 called "The Story of Mel"</a> about a programmer who refused to use even an optimizing assembler because, as he put it, "you never know where it's going to put things." The thing is, Mel's hand-tuned code <em>did</em> run faster. The resistance was not irrational; it was empirically grounded — at the time. But by 1958, over half of all code on IBM machines was already being generated by Fortran. Today compiling by hand would be unthinkable. Certain software engineering programs have even stopped teaching assembly altogether!</p>


            <p>The compiler story is probably the most directly analogous to agents, but the pattern repeats at every rung of the abstraction ladder. Object-oriented programming in the '80s and '90s is another good example. The core idea of OOP was to bundle data and the functions that operate on it into "objects," organized into inheritance hierarchies. The promise was that this would make code more modular and reusable. The criticism, broadly, was that this bundling created more problems than it solved. Inheritance hierarchies tend to get deep and tangled — you change something in a parent class and break five child classes you forgot about. State gets hidden inside objects, making it hard to reason about what a program is actually doing. And the reusability promise often didn't pan out in practice: Joe Armstrong, the creator of Erlang, put it memorably in an <a href="https://www.johndcook.com/blog/2011/07/19/you-wanted-banana/">interview for <em>Coders at Work</em></a> (2009): "You wanted a banana but what you got was a gorilla holding the banana and the entire jungle." Armstrong also wrote an essay called <a href="https://harmful.cat-v.org/software/OO_programming/why_oo_sucks">"Why OO Sucks"</a> where he noted that at the time "criticising OOP was rather like swearing in church" — there was real social pressure to go along with the paradigm. Alexander Stepanov, the guy who created C++'s own Standard Template Library, <a href="https://wiki.c2.com/?ObjectOrientationIsaHoax">called OOP "almost as much of a hoax as Artificial Intelligence"</a>. His argument was that you should start with algorithms, not with class hierarchies — that OOP gets the decomposition backwards. Torvalds called C++ <a href="https://harmful.cat-v.org/software/c++/linus">"a horrible language"</a> and deliberately wrote Git in C, in part to keep C++ programmers away from the project. Dijkstra quipped that <a href="https://www.yegor256.com/2016/08/15/what-is-wrong-object-oriented-programming.html">"object-oriented programming is an exceptionally bad idea which could only have originated in California."</a> But OOP won anyway. Not because the critics were wrong about the problems (inheritance hierarchies <em>are</em> a mess), but because the abstraction made it easier for large teams to build large systems.</p>

            <p>Looking at all of these transitions together, there is a pattern that I think is hard to ignore. First: "it can't be done" (compilers can't match hand-coded assembly). Then: "it's too slow" (Java is 20x slower, GC pauses are unacceptable). Then: "real programmers don't use it" (real programmers use Fortran, real programmers manage their own memory). Then, after the new abstraction wins anyway: "they ruined it" (see: every Agile Manifesto signatory writing essays titled "Agile is Dead" a decade later). I figure we are somewhere between stages two and three with coding agents right now. The objections are partially valid (they do hallucinate, they do sometimes write subtly wrong code), just like early compilers did produce worse assembly, and early Java was genuinely slow. But if history is any indication, the trajectory seems fairly clear. In a sense, every layer of abstraction we ever built did the same thing: we distilled insights about what we understood in programming, taking repetitive tasks and making software to do them for us. LLMs are no different. By training on large amounts of text and code, they compressed a lot of boilerplate and knowledge into a new abstraction layer. The insights might be fuzzier and based on probabilities, but at the end of the day coding <em>is</em> creative and when humans code there is some amount of randomness. Throughout my coding journey, I have been told multiple times by many programmer colleagues and friends that there is a "right way" to code. They all had a different conception of what this meant. In any case, if you are looking, you can find me vibe coding with my buddy Claude.</p>
        </div>

        <footer class="blog-footer">
            <a href="../index.html">← back to home</a> · <a href="index.html">all posts</a>
        </footer>
    </article>
</body>
</html>
